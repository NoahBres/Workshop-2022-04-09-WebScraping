{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python\n",
    "\n",
    "Presentation material: https://github.com/NoahBres/Workshop-2022-04-09-WebScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install requests beautifulsoup4 pandas seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Review\n",
    "\n",
    "HTML = HyperText Markup Language\n",
    "\n",
    "Describes the structure of a web page via tags.\n",
    "\n",
    "Tags are represented like like so:\n",
    "\n",
    "```html\n",
    "<tag>Content</tag>\n",
    "```\n",
    "\n",
    "Tags can have tags nested inside them.\n",
    "\n",
    "```html\n",
    "<tag>\n",
    "  <tag>Content</tag>\n",
    "</tag>\n",
    "```\n",
    "\n",
    "HTML has a number of predefined tags: `body`, `p`, `h1`, `head`, `title`, etc.\n",
    "A full list of tags can be found on MDN: https://developer.mozilla.org/en-US/docs/Web/HTML/Element\n",
    "\n",
    "Tags can also have atrributes describing them:\n",
    "```html\n",
    "<img src=\"/dog-on-log.jpg\" />\n",
    "```\n",
    "\n",
    "Here's a simple HTML page:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Page Title</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "    <button>Click me!</button>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules\n",
    "- BeautifulSoup\n",
    "  - HTML/XML Parser\n",
    "  - Allows us to read/parse HTML/XML\n",
    "- Requests\n",
    "  - HTTP Requests library\n",
    "  - Allows us to fetch data from the web and download webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading a simple page\n",
    "Use the requests library to fetch/download a page and print its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the page\n",
    "Use BeautifulSoup to parse the page and read its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the <p> tags\n",
    "Read all the <p> tags from the page and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[0].get_text()\n",
    "# Same as soup.find('p').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find elements via selectors\n",
    "Request a new page. Find all the elements via selectors instead of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select via tag and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_=\"outer-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real world sample\n",
    "Here's a sandbox we can play around with and run some actual stats.\n",
    "http://books.toscrape.com/catalogue/category/books_1/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"http://books.toscrape.com/catalogue/category/books_1/index.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the HTML that each item has a `product_pod` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = soup.select(\".product_pod\")\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all the titles from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [x.text for x in soup.select(\".product_pod h3\")]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the prices from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [float(x.text[1:]) for x in soup.select('.price_color')]\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instock = [x.text.strip() for x in soup.select(\".instock\")]\n",
    "instock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all the pages\n",
    "Loop through the urls by page number and repeat the same scraping method.\n",
    "This can be improved through threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 2-51\n",
    "for page in range(2,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/category/books_1/page-\" + str(page) +\".html\"\n",
    "    results = requests.get(url)\n",
    "    soup = BeautifulSoup(results.content, 'html.parser')\n",
    "    titles.extend([x.text for x in soup.select(\".product_pod h3\")])\n",
    "    prices.extend([float(x.text[1:]) for x in soup.select('.price_color')])\n",
    "    instock.extend([x.text.strip() for x in soup.select(\".instock\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making use of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "d = {\"title\": titles, \"price\": prices, \"availability\": instock}\n",
    "books = pandas.DataFrame(data=d)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['price'].describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seaborn.set(style = 'darkgrid', color_codes = True)\n",
    "f, ax = plt.subplots(figsize=(13, 3))\n",
    "seaborn.despine(f, left=True, bottom=True)\n",
    "\n",
    "boxplt = seaborn.boxplot(x=books[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTS Fetch Intercept Example\n",
    "\n",
    "- Right click the selected request\n",
    "- Select \"Copy as cURL\"\n",
    "- Open up [https://curlconverter.com](https://curlconverter.com)\n",
    "- Paste the copied cURL into the \"cURL\" field\n",
    "- Go to the Python tab and it'll show the equivalent Python code for this request with all the headers\n",
    "\n",
    "ðŸš¨ THE FOLLOWING SAMPLE WON'T WORK FOR YOUðŸš¨\n",
    "\n",
    "The RTS application seems to have an expiration for the access time. You have to copy the cURL and use your own headers for this to work.\n",
    "\n",
    "Otherwise, the request will return `No API access permitted`\n",
    "\n",
    "The following sample will be expired.\n",
    "You can probably reverse engineer how this works on your own if you care. I suspect it's calling the gettime method and then using the data from that.\n",
    "\n",
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'sec-ch-ua': '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"97\", \"Chromium\";v=\"97\"',\n",
    "    'Accept': 'application/json',\n",
    "    'X-Date': 'Fri, 08 Apr 2022 23:36:00 GMT',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',\n",
    "    'X-Request-ID': '4710baeb042156e238641cedf72802eddc82db50a5a3638782dcca0e9c2c4bbb',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Referer': 'https://www.riderts.app/map',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'requestType': 'getvehicles',\n",
    "    'rt': '20',\n",
    "    'key': 'Qskvu4Z5JDwGEVswqdAVkiA5B',\n",
    "    'format': 'json',\n",
    "    'xtime': '1649460961927',\n",
    "}\n",
    "\n",
    "response = requests.get('https://www.riderts.app/bustime/api/v3/getvehicles', headers=headers, params=params)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
